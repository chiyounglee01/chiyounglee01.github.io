---
layout: default
modal-id: 3
date: 2023-08-10
img: housing_prices.png
alt: image-alt
project-date: Aug 2024
client: Kaggle
category: Regression
description: A regression based Data Science project to predict housing prices in Ames Iowa.  Click <a href="https://github.com/chiyounglee01/predicting_house_prices" target="blank">here</a> for the project's Github repository. <br></br><h1><b>Overview</b></h1><br></br><p>The goal of this project was to develop a predictive model for estimating house prices based on a range of features provided in the dataset. This project, hosted on Kaggle, involved applying advanced regression techniques to a dataset of house sale prices in Ames, Iowa. The challenge was to build a model that can accurately predict house prices based on both numeric and categorical features. The project used <b>Python</b> code.</p><p>The tools used include <b>Pandas</b> (for data manipulating), <b>Scikit-Learn</b> (ecosystem for various regression models), and <b>Seaborn</b> (for visualization). The models used for the final regression were <b>Elastic Net</b>, <b>Ridge Regression</b>, <b>Gradient Boosting Regression</b>, and <b>Random Forest Regression</b>.</p><br></br><p><b>With this project I wanted to get familiar with regression techniques and answer the following question:</b></p><p>*What <b>log nrmse</b> would I get for the Sale Price predictions by using various regression techniques?</p><br></br><h1>General Summary</h1><br></br><p>There were 79 features in the dataset. There were both numeric and categorical features in the dataset. For the numeric features, I dropped features which had less than an absolute correlation of 0.25 with the <b>Sale Price</b>. I also dropped features where more than 80 percent of the rows were missing data. For categorical features, I used my domain knowledge in real estate to drop features like <b>Masonry Veneer Type</b> as it is a feature does not affect house sales in real life.</p><img src="https://raw.githubusercontent.com/chiyounglee01/images/main/percentage_missing.png"><p>For rows, outliers for features highly correlated with <b>Sales Price</b> like <b>Overall Quality</b> were removed. Depending on the column, I imputed missing row data with the mean, mode, 0, or ‘None’.</p><img src="https://raw.githubusercontent.com/chiyounglee01/images/main/Sales_Price_vs_Overall_Quality.png"><br></br><p>Finally, I fit the train data with the following models <b>Elastic Net</b>, <b>Ridge Regression</b>, <b>Gradient Boost Regression</b>, and <b>Random Forest Regression</b>. I used the <b>VotingRegressor()</b> function in scikit-learn to combine all 4 models. <b>VotingRegressor()</b> improves overall accuracy by averaging the predictions of each model. Extra weight was put on the <b>Gradient Boost</b> model as it had the smallest root mean squared error out of all the models.</p><p><img src="https://raw.githubusercontent.com/chiyounglee01/images/main/voting_regressor.png"><br></br><h1><b>MAIN FINDINGS</b></h1><p>The final <b>log nrmse</b> for this model was <b>0.17</b>. It was satisfactory as it was below 0.2, but it was not great as it was over 0.1. Since the dataset had 79 features, choosing the right features for the model is crucial and using just correlation and real estate knowledge to choose features may have limitations.</p><img src="https://raw.githubusercontent.com/chiyounglee01/images/main/submission.png">




---
